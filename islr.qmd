---
title: "Introduction to Statistcal Learing"
subtitle: "with Applications in R (ISLR) abstracted to Biomarker Discovery and Prediction"
author: "Jeffrey Long"
format: html
editor: visual
---

## A Quarto Experiment

Quarto enables you to weave together content and executable code into a finished document.

ISLR second edition is available for the author in [a free pdf format](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf).

This is my biomarker experiment with Quarto and ISLR.

## History

The Elements of Statistical Learning (ESL, by Hastie, Tibshirani, and Friedman) --- was published in 2001. The authors of ISLR2 (the resource of this experiment) are Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.

## Contents

1.  Introduction
2.  Statistical Learning
3.  Linear Regression
4.  Classification
5.  Resampling Methods
6.  Linear Model Selection and Regularization
7.  Moving Beyond Linearity
8.  Tree-Based Methods
9.  Support Vector Machines
10. Deep Learning
11. Survival Analysis and Censored Data
12. Unsupervised Learning
13. Multiple Testing

## Introduction

Statistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for pre- dicting, or estimating, an output based on one or more inputs. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and struc- ture from such data.

**The `Wage` data** involves predicting a continuous or quantitative output value. This is often referred to as a regression problem. However, in certain cases we may instead wish to predict a non-numerical value---that is, a categorical or qualitative output.

An example of predicting non-numerical values, such as up/down is explored with **the `Smarket` data** are the Standard & Poor's 500 (S&P) stock index over a 5-year period between 2001 and 2005.

Imagine the first steps of a search for biomarkers in gene expression data. We might have demographic information for a number of patients with an indication. We may wish to understand which types of patients are similar to each other by grouping individuals according to observed characteristics. This is a *clustering* problem. Unlike `Wage` and `Smarket` data, here we are not trying to predict an output variable. We will use **the NCI60 data set** to determine groups or clusters based on 6,830 gene expression measurements for 64 cell lines.

In this particular data set, it turns out that the cell lines correspond to 14 different types of cancer. There is clear evidence that cell lines with the same cancer type tend to be located near each other in clustering methods.

We will use *n* to represent the number of distinct data points, or observations, in our sample. We will let *p* denote the number of variables that are available for use in making predictions. For example, the `Wage` data set consists of 11 variables for 3,000 people, so we have *n* = 3,000 observations and *p* = 11 variables (such as year, age, race, and more). We indicate variable names using the font: `Variable Name`.

We will let $x_{ij}$ represent the value of the *j*th variable for the *i*th observation, where *i* = 1,2,...,*n* and *j* = 1,2,...,*p*. Throughout this experiment, *i* will be used to index the samples or observations (from 1 to *n*) and j will be used to index the variables (from 1 to *p*). We let $\chi$ denote an $n × p$ matrix whose (*i*, *j*)th element is $x_{ij}$ .

The product of **A** and **B** is denoted **AB**. The (*i*,*j*)th element of **AB** is computed by multiplying each element of the *i*th row of **A** by the corresponding element of the *j*th column of **B**. That is, $(AB)_{ij} = \Sigma^d_{k=1} a_{ik}b_{kj}$. As an example,

consider $A = \begin{pmatrix}1 & 2\\3 & 4\end{pmatrix}$ and $B = \begin{pmatrix}5 & 6\\7 & 8\end{pmatrix}$ .

$AB = \begin{pmatrix}1 & 2\\3 & 4\end{pmatrix}\begin{pmatrix}5 & 6\\7 & 8\end{pmatrix} = \begin{pmatrix}1*5+2*7 & 1*6+2*8\\3*5+4*7 & 3*6+4*8\end{pmatrix} = \begin{pmatrix}19 & 22\\43 & 50\end{pmatrix}$

Note that this operation produces an $r × s$ matrix. It is only possible to compute **AB** if the number of columns of **A** is the same as the number of rows of **B**.

```{r}
# install.packages("ISLR2")
library(ISLR2)
packageDescription("ISLR2")

```

The website for ISLR2 is [www.statlearning.com](https://www.statlearning.com/).

## Statistical Learning

The input variables are typically denoted using the symbol X, with a subscript to distinguish them. The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables, and is typically denoted using the symbol Y.

Suppose that we observe a quantitative response Y and p different predictors, X1, X2, ... , Xp. We assume that there is some relationship between Y and X = (X1,X2,...,Xp), which can be written in the very general form

$Y =f(X)+\epsilon$.

Here f is some fixed but unknown function of $X_1, \hdots, X_p$ , and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the systematic information that $X$ provides about $Y$ . However, the function $f$ that connects the input variable to the output variable is in general unknown. In this situation one must estimate $f$ based on the observed points. Overall, the errors have approximately mean zero. The function $f$ may involve more than one input variable.

### Why Estimate f?

There are two main reasons that we may wish to estimate f: prediction and inference. We discuss each in turn.

#### Prediction

In many situations, a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. In this setting, since the error term averages to zero, we can predict $Y$ using

$\hat{Y} = f(X)$ ,

where $\hat{f}$ represents our estimate for $f$ , and $\hat{Y}$ represents the resulting prediction for $Y$ . In this setting, $\hat{f}$ is often treated as a *black box*, in the sense that one is not typically concerned with the exact form of $\hat{f}$ , provided that it yields accurate predictions for $Y$ .

The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the *reducible error* and the *irreducible error*. In general, $\hat{f}$ will not be a perfect estimate for \$f\$, and this inaccuracy will introduce some error. This error is reducible because we can potentially improve the accuracy of $\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$ . However, even if it were possible to form a perfect estimate for $f$ , so that our estimated response took the form $\hat{Y} = f(X)$ , our prediction would still have some error in it! This is because $Y$ is also a function of $\epsilon$ , which, by definition, cannot be predicted using $X$ . Therefore, variability associated with $\epsilon$ also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate $f$ , we cannot reduce the error introduced by $\epsilon$ .

#### Inference

We are often interested in understanding the association between $Y$ and $X_1,...,X_p$ . In this situation we wish to estimate $f$ , but our goal is not necessarily to make predictions for $Y$ . Now $\hat{f}$ cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:

• Which predictors are associated with the response? It is often the case that only a small fraction of the available predictors are substantially associated with $Y$ . Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application.

• What is the relationship between the response and each predictor? Some predictors may have a positive relationship with $Y$ , in the sense that larger values of the predictor are associated with larger values of $Y$ . Other predictors may have the opposite relationship. Depending on the complexity of $f$ , the relationship between the response and a given predictor may also depend on the values of the other predictors.

• Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Historically, most methods for estimating $f$ have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.

### How Do We Estimate $f$ ?

Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function $f$ .

-   training data - observations and responses

-   parametric methods - model-based approach that makes an assumption about $f$ i.e. *linear model*, then fits or trains the model i.e. *least squares*. Beware of overfitting.

-   non-parametric - methods do not make explicit assumptions about the functional form of $f$ . Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for $f$ , they have the potential to accurately fit a wider range of possible shapes for $f$ . Any parametric approach brings with it the possibility that the functional form used to estimate $f$ is very different from the true $f$ , in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of $f$ is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$ .

### The Trade-Off Between Prediction Accuracy and Model Interpretability

Some methods such as linear regression are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate $f$ , such as linear function. Other methods such as thin plate splines are considerably more flexible because they can generate a much wider range of possible shapes to estimate $f$ .

![A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods. In general, as the flexibility of a method increases, its interpretability decreases.](interpretability_v_flexibility.png)

However, if we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between $Y$ and $X_1,X_2,…,X_p$ .

Least squares linear regression, is relatively inflexible but is quite interpretable. The lasso, relies upon the linear model but uses an alternative fitting procedure for estimating the coefficients $\beta_0, \beta_1, . . . , \beta_p$ . The *lasso* is more restrictive in es- timating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors---namely, those with nonzero coefficient estimates. Generalized additive models (GAMs) instead extend the linear model to allow for certain non-linear relationships. Consequently, GAMs are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve. Finally, fully non-linear methods such as bagging, boosting, support vector machines with non-linear kernels, and neural networks (deep learning), are highly flexible approaches that are harder to interpret.

When prediction is the goal, one may suspect the most flexible method is the best choice as it would give the most accurate prediction. However, overfitting often leads us to less flexible methods with improved performance.

### Supervised Versus Unsupervised Learning

In supervised learning problems, for each observation of the predictor measurement(s) $x_i, i = 1, . . . , n$ there is an associated response measurement $y_i$ . We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression,as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain.

Unsupervised learning describes the somewhat more challenging situation in which for every observation $i = 1,…,n$ , we observe a vector of measurements $x_i$ but no associated response $y_i$ . It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis.

One statistical learning tool that we may use in this setting is cluster analysis, or clustering. The goal of cluster analysis is to ascertain, on the basis of $x_1,…,x_n$ , whether the observations fall into relatively distinct groups. Identifying such groups can be of interest because it might be that the groups differ with respect to some property of interest.

### Regression Versus Classification Problems

Variables can be characterized as either quantitative or qualitative (also known as categorical). Quantitative variables take on numerical values. In contrast, qualitative variables take on values in one of K different classes, or categories. We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or binary) response. K-nearest neighbors and boosting can be used in the case of either quantitative or qualitative responses.

We tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important.

## Assessing Model Accuracy

No one method dominates all others over all possible data sets. Hence it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.

### Measuring Quality of Fit

In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by

$MSE = \frac{1}{n}\Sigma^n_{i=1}(y_i-\hat{f}(x_i))^2$ ,

where $\hat{f}(x_i)$ is the prediction that $\hat{f}$ gives for the $i$th observation.

The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.

The MSE in is computed using the training data that was used tofit the model, and so should more accurately be referred to as the training MSE. But in general, we do not really care how well the method works training on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. Suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.

We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE. If we had a large number of test observations, we could compute

$Ave(y_0 - \hat{f}(x_0))^2$ ,

the average squared prediction error for these test observations $(x_0,y_0)$ . We'd like to select the model for which this quantity is as small as possible.

The degrees of freedom is a quantity that summarizes the flexibility of a curve. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. Regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.

Plotting of MSE versus degrees of freedom as model flexibility can guide model selection. Cross-validation can estimate the minimum test MSE using the training data.

### The Bias-Variance Trade-Off

It is possible to show that the **expected test MSE**, for a given value $x_0$ , can always be decomposed into the sum of three fundamental quantities: the variance of \$f(x_0)\$, the squared bias of $f(x_0)$ and the variance of the error terms $\epsilon$ .

$E(y_0-\hat{f}(x_0))2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))^2 + Var(\epsilon)$ .

we need to select a statistical learning method that simultaneously achieves low variance and low bias. Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below $Var(\epsilon)$ , the irreducible error.

Variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance. Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias.

As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.

![Squared bias (blue curve), variance (orange curve), Var(ε) (dashed line), and test MSE (red curve) for the three data sets. The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.](var_bias_error.png)

In a real-life situation in which $f$ is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the **bias-variance trade-off** in mind.

### The Classification Setting

When response variable $y$ is qualitative, the most common approach for quantifying the accuracy of our estimate $\hat{f}$ is the training error rate, the proportion of mistakes that are made if we apply our estimate $\hat{f}$ to the training observations:

$\frac{1}{n}\Sigma^n_{i=1}I(y_i!=\hat{y}_i)$ .

Here $\hat{y}_i$ is the predicted class label for the $i$ th observation using $f$ . And $I(y_i != \hat{y}_i)$ is an *indicator variable* that equals 1 if $y_i ̸!= \hat{y}_i$ and zero if $y_i = \hat{y}_i$ . If $I(y_i ̸!= \hat{y}_i) = 0$ then the $i$ th observation was classified correctly by our classification method; otherwise it was misclassified. Hence the training error computes the fraction of incorrect classifications.

The test error rate associated with a set of test observations of the form $(x_0, y_0)$ is given by

$Ave(I(y_0 ̸!=ˆ\hat{y}_0))$ ,

where $\hat{y}_0$ is the predicted class label that results from applying the classifier to the test observation with predictor $x_0$. A good classifier is one for which the test error is smallest.

### The Bayes Classifier

The test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector $x_0$ to the class $j$ for which

$Pr(Y = j|X = x_0)$

is largest. This conditional probability is the probability that $Y = j$ , given the observed predictor vector $x_0$ . This very simple classifier is called the *Bayes classifier*. In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if $Pr(Y = 1|X = x_0) > 0.5$ , and class two otherwise.

![A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class.](bayes_classifier.png)

The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. Since the Bayes classifier will always choose the class for which is largest, the error rate will be $1−max_j Pr(Y = j|X = x_0) at X = x_0$ . In general, the overall Bayes error rate is given by

$1−E(max_jPr(Y =j|X))$ ,

where the expectation averages the probability over all possible values of $X$ . If the Bayes error rate is greater than zero, the classes overlap in the true population so $max_j Pr(Y = j|X = x_0) < 1$ for some values of $x_0$ . The Bayes error rate is analogous to the irreducible error.

### K-Nearest Neighbors

In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of $Y$ given $X$ , and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods. Many approaches attempt to estimate the conditional distribution of $Y$ given $X$ , and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier. Given a positive integer $K$ and a test observation $x_0$ , the KNN classifier first identifies the $K$ points in the training data that are closest to $x_0$ , represented by $N_0$ . It then estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$ :

$Pr(Y =j|X =x_0)= \frac{1}{K} \Sigma_{i\element N_0} I(y_i =j)$ .

Finally, KNN classifies the test observation $x_0$ to the class with the largest probability.

![Example of k-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle). -from Wikipedia.](KnnClassification.svg)

![The black curve indicates the KNN decision boundary on data using K = 10. The Bayes decision boundary is shown as a purple dashed line. The KNN and Bayes decision boundaries are very similar.](knn_k10.png)

![A comparison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on data. With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line.](knn_k1_k100.png)

![The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data, as the level of flexibility (assessed using 1/K on the log scale) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set.](knn_training_error_rate.png)

The test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately K = 10) before increasing again when the method becomes excessively flexible and overfits.

## Lab in R

Posit (formerly known as RStudio), provides an R integrated development environment (IDE).

```{r}
# Basic Commands
x <- c(1, 3, 2, 5)
x = c(1, 6, 2)
y = c(1, 4, 3)
length(x)
length(y)
x+y
ls()
rm(x, y)
ls()
x = c(1, 6, 2)
y = c(1, 4, 3)
rm(list = ls())
?matrix
x <- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)
x
matrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)
sqrt(x)
x <- rnorm(50)
y <- x + rnorm(50, mean = 50, sd = .1)
cor(x, y)
set.seed(1303)
rnorm(50)
set.seed(3)
y <- rnorm(100)
mean(y)
var(y)
sqrt(var(y)) # SD
sd(y)
# Graphics
x <- rnorm(100)
y <- rnorm(100)
plot(x, y)
plot(x, y, xlab = "this is the x-axis",
ylab = "this is the y-axis", main = "Plot of X vs Y")
pdf("Figure.pdf")
plot(x, y, col = "green")
dev.off()
x <- seq(1, 10)
x
x <- 1:10
x
x <- seq(-pi, pi, length = 50)
x
y <- x
f <- outer(x, y, function(x, y) cos(y) / (1 + x^2))
contour(x, y, f)
fa <- (f - t(f)) / 2
contour(x, y, fa, nlevels = 15)
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta = 30)
persp(x, y, fa, theta = 30, phi = 20)
persp(x, y, fa, theta = 30, phi = 70)
persp(x, y, fa, theta = 30, phi = 40)
# Indexing Data
A <- matrix(1:16, 4, 4)
A
A[2,3]
A[c(1, 3), c(2, 4)]
A[1:3, 2:4]
A[1:2, ]
A[-c(1, 3), ]
dim(A)
# Loading Data
library(ISLR2)
head(Auto)
write.table(Auto, "Auto.data")
Auto <- read.table("Auto.data")
Auto <- read.table("Auto.data", header = T, na.strings = "?", stringsAsFactors = T)
# Auto <- read.csv("Auto.csv", na.strings = "?", stringsAsFactors = T)
Auto <- na.omit(Auto)
names(Auto)
# Additional Graphical and Numerical Summaries
plot(Auto$cylinders, Auto$mpg)
attach(Auto)
plot(cylinders, mpg)
cylinders <- as.factor(cylinders)
plot(cylinders, mpg)
plot(cylinders, mpg, col = "red")
plot(cylinders, mpg, col = "red", varwidth = T)
plot(cylinders, mpg, col = "red", varwidth = T, horizontal = T)
plot(cylinders, mpg, col = "red", varwidth = T, xlab = "cylinders", ylab = "MPG")
hist(mpg)
hist(mpg, col = 2)
hist(mpg, col = 2, breaks = 15)
plot(horsepower, mpg)
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
      pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
pairs(iris, log = 1:4, # log the first four
      main = "Lengths and Widths in [log]", line.main=1.5, oma=c(2,2,3,2))
names(iris)
plot(iris$Sepal.Length,iris$Petal.Length)
summary(iris)

college <- College
names(college)
rownames(college)
summary(college)
pairs(college[,1:5])
?Boston
```

## Linear Regression

Linear regression is a useful tool for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$ . It assumes that there is approximately a linear relationship between $X$ and $Y$ .

$Y ≈ β_0 + β_1X$ .

Patient survival rate may be approximately modeled as response to biomarker A quantity. Then we regress patient survival rate onto biomarker A quantity by fitting the model

$patient\_survival\_rate ≈ β_0 + β_1 * biomarker\_A\_quantity$ .

$β_0$ and $β_1$ are the intercept and slope terms, unknown constants. They are the model coefficients or parameters. These will be derived by the training data, then we may predict patient survival rate of a biomarker A quantity by

$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$ ,

where $\hat{y}$ indicates a prediction of $Y$ on the basis of $X = x$.

### Estimating Coefficients

In practice, $\beta_0$ and $\beta_1$ are unknown. So we must use data to estimate the coefficients. Let

$(x_1,y_1), (x_2,y_2),…, (x_n,y_n)$

represent n observation pairs, each of which consists of a measurement of $X$ and a measurement of $Y$ . Our goal is to obtain coefficient estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ such that the linear model fits the available data well---that is, so that $y_i ≈ \hat{\beta_0} + \hat{\beta_1}x_i$ for \$i = 1, . . . , n\$. In other words, we want to find an intercept $\hat{\beta_0}$ and a slope $\hat{\beta_1}$ such that the resulting line is as close as possible to the data points. There are a number of ways of measuring closeness. However, by far the most common approach involves minimizing the least squares criterion.

Let $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i$ be the prediction for $Y$ based on the $i$th value of $X$ . Then $e_i = y_i − \hat{y_i}$ represents the $i$ th residual---this is the difference between the $i$ th observed response value and the $i$ th response value that is predicted by our linear model. We define the residual sum of squares (RSS) as

$RSS=e^2_1 +e^2_2 +···+e^2_n$ ,

or equivalently as

$RSS=(y_1−\hat{\beta_0}−\hat{\beta_1}x_1)^2+(y_2−\hat{\beta_0}−\hat{\beta_1}x_2)^2+···+(y_n−\hat{\beta_0}−\hat{\beta_1}x_n)^2$.

The least squares approach chooses $\hat{\beta_0}$ and $\hat{\beta_1}$ to minimize the RSS. Using some calculus, one can show that the minimizers are

$\hat{\beta_1}=\frac{\Sigma^n_{i=1}(x_i − \bar{x})(y_i − \bar{y})}{\Sigma^n_{i=1}(x_i - \bar{x})^2}$ ,

$\hat{\beta_0} = \bar{y}-\hat{\beta_1}\bar{x}$ ,

where $\bar{y} = \frac{1}{n}\Sigma^n_{i=1}y_i$ and $\bar{x} = \frac{1}{n}\Sigma^n_{i=1}x_i$ are the sample means. In other words, this defines the least squares coefficient estimates for simple linear regression.

### Assessing the Accuracy of the Coefficient Estimates

We assume that the \_true\_ relationship between $X$ and $Y$ takes the form $Y = f(X) + \epsilon$ for some unknown function \$f\$, where $\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as

$Y = \beta_0 + \beta_1 X + \epsilon$ .

Here $\beta_0$ is the intercept term---that is, the expected value of $Y$ when $X = 0$ , and $\beta_1$ is the slope---the average increase in $Y$ associated with a one-unit increase in $X$ . The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$ , and there may be measurement error. We typically assume that the error term is independent of $X$ .

This model defines the *population regression line*, which is the best linear approximation to the true relationship between $X$ and $Y$ . The least squares regression coefficient estimates characterize the least squares line.

The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates. In other words, in real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved.

A reasonable estimate of the population mean $\hat{\mu} = \bar{y}$ , is the sample mean, where $\bar{y} = \frac{1}{n}\Sigma^n_{i=1}y_i$ . In the same way, the unknown coefficients $\beta_0$ and $\beta_1$ in linear regression define the population regression line. We seek to estimate these unknown coefficients using $\hat{\beta_0}$ and $\hat{\beta_1}$ . These coefficient estimates define the least squares line.

To quantify how accurate the sample mean $\hat{\mu}$ 's estimation is of $\mu$ , we compute the standard error of $SE(\hat{\mu})$ .

$Var(\hat{\mu}) = SE(\hat{\mu})^2 = \frac{\sigma^2}{n}$ ,

where $\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$ . This holds provided that the $n$ observations are uncorrelated. The standard error tells us the average amount that this estimate $\hat{\mu}$ differs from the actual value of $\mu$ . Also note, this deviation shrinks with $n$ ---the more observations we have, the smaller the standard error of $\hat{\mu}$ . In a similar vein, we can wonder how close $\hat{\beta_0}$ and $\hat{\beta_1}$ are to the true values $\beta_0$ and $\beta_1$ . To compute the standard errors associated with $\hat{\beta_0}$ and $\hat{\beta_1}$ , we use the following formulas:

$SE(\hat{\beta_0})^2 = \sigma^2\big[\frac{1}{n} + \frac{\bar{x}^2}{\Sigma^n_{i=1}(x_i-\bar{x})^2}\big]$ ,

$SE(\hat{\beta_1})^2 = \sigma^2\big[\frac{1}{n} + \frac{\sigma^2}{\Sigma^n_{i=1}(x_i-\bar{x})^2}\big]$ ,

where $\sigma^2 = Var(\epsilon)$ . For these formulas to be strictly valid, we need to assume that the errors $\epsilon_i$ for each observation have common variance $\sigma^2$ and are uncorrelated. Notice in the formula that $SE(\hat{\beta_1})$ is smaller when the $x_i$ are more spread out; intuitively we have more *leverage* to estimate a slope when this is the case. We also see that $SE(\hat{\beta_0})$ would be the same as $SE(\hat{\mu})$ if $\bar{x}$ were zero (in which case $\hat{\beta_0}$ would be equal to $\bar{y}$ . In general, $\sigma^2$ is not known, but can be estimated from the data. This estimate of $\sigma$ is known as the *residual standard error* , and is given by $RSE = \sqrt{RSS/(n-2)}$ .

Standard errors can be used to compute *confidence intervals*. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. A 95% confidence interval has the following prop- erty: if we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter. For linear regression, the 95% confidence interval for $\beta_1$ approximately takes the form

$\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1})$

That is there is approximately a 95% chance that the interval

$\big[\hat{\beta_1} - 2 \cdot SE(\hat{\beta_1}), \hat{\beta_1} + 2 \cdot SE(\hat{\beta_1)}\big]$

will contain the true value of $\beta_1$ . Similarly, a confidence interval for $\beta_0$ approximately takes the form

$\hat{\beta_0} \pm 2 \cdot SE(\hat{\beta_0})$ .
